{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a534b1-5286-4b87-81a2-0ea2eb6558d8",
   "metadata": {},
   "source": [
    "# Red neuronal para el data-set de animales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f70256-3a58-44cd-9b1b-937ba924688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43925464-b740-4e92-b251-4293c0691de0",
   "metadata": {},
   "source": [
    "## 1. Importar los conjuntos de datos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57092827-99a8-4f81-8ae9-b381e5568e04",
   "metadata": {},
   "source": [
    "Los conjuntos X contienen los atributos que se utilizan de entrada para una red neuronal. Los conjuntos Y tienen al atributo $\\textit{type}$, que es el atributo cuyo valor la red neuronal debe inferir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8b5d51-e3ad-4da5-9fc8-93db6d6e02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_train_test():\n",
    "    X_train = pd.read_csv('X_train.csv',index_col=0)\n",
    "    X_test = pd.read_csv('X_test.csv',index_col=0)\n",
    "\n",
    "    y_train = pd.read_csv('y_train.csv',index_col=0)\n",
    "    y_train = pd.DataFrame({'type':np.array(y_train.index)\n",
    "                      })\n",
    "\n",
    "    y_test = pd.read_csv('y_test.csv',index_col=0)\n",
    "    y_test = pd.DataFrame({'type':np.array(y_test.index)\n",
    "                       })\n",
    "    return (X_train,X_test,y_train,y_test)\n",
    "\n",
    "X_train,X_test,y_train,y_test = cargar_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb668b39-e6ad-4436-821f-2966e0c02134",
   "metadata": {},
   "source": [
    "Puedes descomentar las siguientes celdas para visualizar estos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc81c57-0df4-462c-aca0-977d0ce90390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcaf83a3-ecdf-42d7-aa4b-f0939591fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc1d332-0afd-41be-bdd4-a48abd1ac748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4195a6b5-8c24-4341-a194-2ff21b14061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb44f15-1e64-4937-b7aa-6dd87f58b734",
   "metadata": {},
   "source": [
    "## 2. Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c40a2c-59a7-4879-bc39-25e08d177d1a",
   "metadata": {},
   "source": [
    "Los conjuntos X e Y se normalizan para facilitarle el trabajo al algoritmo de optimización y las funciones de activación. La técnica de normalización utilizada se llama $\\textit{min-max}$. Para cada atributo $a$ de los datos, se mapea cada valor $v \\mapsto v / max(a)$, donde $ max(a)$ es el maximo valor que existe en ese atributo. De esta forma, cuando $v=max(a)$, el valor normalizado sera 1. Es decir, todos los valores estaran en el rango $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e022d72-a051-42f6-b0ce-be793a88f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_datos(X_train,X_test,y_train,y_test):\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "\n",
    "    X_train = X_train / np.amax(np.concatenate((X_train, X_test), axis=0), axis=0)\n",
    "    X_test = X_test / np.amax(np.concatenate((X_train, X_test), axis=0), axis=0)\n",
    "    y_train = y_train / 7  \n",
    "    y_test = y_test / 7 \n",
    "\n",
    "    return (X_train,X_test,y_train,y_test)\n",
    "\n",
    "X_train,X_test,y_train,y_test = preparar_datos(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d46882a9-2ea5-4141-8746-ddaca4b7d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee11cf9-308f-432c-b6fe-139956f04419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64060f0c-b0b6-415a-868c-6e5b6dd94eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaab2ce-70ed-4aae-9da2-6d6af8464522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3866e7f5-8bf6-42ce-b027-8ec424ccd2e3",
   "metadata": {},
   "source": [
    "## 3. Importar la inicialización de pesos y bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb390b8d-ad4d-42f8-96f1-34ad2e55e03c",
   "metadata": {},
   "source": [
    "Luego de un muestreo de muchos entrenamientos de la red neuronal, con una inicialización aleatoria de pesos y bias, se encontraron estos parametros como los que lograban el mejor valor de accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5fecd2-f9ec-4cd0-b650-daf61cbeb44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importar_pesos_bias():\n",
    "    weights_input_hidden = np.loadtxt('initialized_weights_input_hidden.csv', delimiter=',')\n",
    "    weights_hidden_output= np.loadtxt('initialized_weights_hidden_output.csv', delimiter=',')\n",
    "    bias_hidden= np.array([np.loadtxt('initialized_bias_hidden.csv', delimiter=',')])\n",
    "    bias_output= np.array([np.loadtxt('initialized_bias_output.csv', delimiter=',')])\n",
    "    return (weights_input_hidden,weights_hidden_output,bias_hidden,bias_output)\n",
    "weights_input_hidden,weights_hidden_output,bias_hidden,bias_output = importar_pesos_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb85fa-5487-4342-8e81-d08a5c305dfa",
   "metadata": {},
   "source": [
    "Puedes descomentar las siguientes celdas para visualizar estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3905ea3e-0716-4d74-a848-a88b0443a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_input_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e8b2df2-621b-4b00-996b-fd6d2cd19ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_hidden_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1ce679f-16a4-47c0-a1d0-68d7fb693de5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bias_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "266a1eac-1128-4c61-b698-d20b72898463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bias_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80927a-d9a0-4b04-b2f0-8c041cd0c6f6",
   "metadata": {},
   "source": [
    "## 4. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e411a8-c3c2-477e-964c-a6f74212ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion de activacion sigmoide\n",
    "def sigmoide(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la funcion de activacion sigmoide\n",
    "def sigmoide_derivada(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class PerceptronMulticapa:\n",
    "    def __init__(self, input_size, hidden_size, output_size,weights_input_hidden=None,\n",
    "                weights_hidden_output=None,bias_hidden=None,bias_output=None):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Inicializacion de pesos y bias\n",
    "        # Estos parametros iran cambiando a medida que se optimizen\n",
    "        if weights_input_hidden is None:\n",
    "            self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        else:\n",
    "            self.weights_input_hidden = weights_input_hidden\n",
    "            \n",
    "        if weights_hidden_output is None:\n",
    "            self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "        else:\n",
    "            self.weights_hidden_output = weights_hidden_output\n",
    "            \n",
    "        if bias_hidden is None:\n",
    "            self.bias_hidden = np.random.rand(1, hidden_size)\n",
    "        else:\n",
    "            self.bias_hidden = bias_hidden\n",
    "\n",
    "        if bias_output is None:\n",
    "                self.bias_output = np.random.rand(1, output_size)\n",
    "        else:\n",
    "            self.bias_output = bias_output\n",
    "\n",
    "\n",
    "        # Se guardan las inicializaciones de pesos y bias en atributos diferentes\n",
    "        self.initialized_weights_input_hidden = self.weights_input_hidden.copy()\n",
    "        self.initialized_weights_hidden_output = self.weights_hidden_output.copy()\n",
    "        self.initialized_bias_hidden = self.bias_hidden.copy()\n",
    "        self.initialized_bias_output = self.bias_output.copy()\n",
    "        \n",
    "        \n",
    "    def pensar(self, X):\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = sigmoide(self.hidden_input)\n",
    "        self.output = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        return self.output\n",
    "        \n",
    "    def retropropagacion(self, X, y, output, ratio_aprendizaje):\n",
    "        # Retropropagacion\n",
    "        error = y - output\n",
    "        d_output = error\n",
    "        d_hidden = np.dot(d_output, self.weights_hidden_output.T) * sigmoide_derivada(self.hidden_output)\n",
    "        \n",
    "        # Actualizacion de pesos y bias\n",
    "        self.weights_hidden_output += np.dot(self.hidden_output.T, d_output) * ratio_aprendizaje\n",
    "        self.weights_input_hidden += np.dot(X.T, d_hidden) * ratio_aprendizaje\n",
    "        self.bias_output += np.sum(d_output, axis=0, keepdims=True) * ratio_aprendizaje\n",
    "        self.bias_hidden += np.sum(d_hidden, axis=0, keepdims=True) * ratio_aprendizaje\n",
    "\n",
    "    def predecir(self, X):\n",
    "        return self.pensar(X)\n",
    "    \n",
    "    def entrenar(self, X, y, epocas, ratio_aprendizaje,debug=False,intervalo_de_epoca = 100):\n",
    "        for epoca in range(epocas):\n",
    "            output = self.pensar(X)\n",
    "            \n",
    "            self.retropropagacion(X, y, output, ratio_aprendizaje)\n",
    "            \n",
    "            perdida = np.mean(np.square(y - output))\n",
    "            if debug and epoca % intervalo_de_epoca == 0:\n",
    "                print(f'Epoca {epoca}, Perdida: {perdida:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23e452-338f-455f-9c7a-baa581a42472",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f1278a2-37a7-4836-b953-c9385509b86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 0, Perdida: 997.9707\n",
      "Epoca 100, Perdida: 0.0899\n",
      "Epoca 200, Perdida: 0.0898\n",
      "Epoca 300, Perdida: 0.0897\n",
      "Epoca 400, Perdida: 0.0897\n",
      "Epoca 500, Perdida: 0.0896\n",
      "Epoca 600, Perdida: 0.0896\n",
      "Epoca 700, Perdida: 0.0895\n",
      "Epoca 800, Perdida: 0.0895\n",
      "Epoca 900, Perdida: 0.0895\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = cargar_train_test()\n",
    "X_train,X_test,y_train,y_test = preparar_datos(X_train,X_test,y_train,y_test)\n",
    "weights_input_hidden,weights_hidden_output,bias_hidden,bias_output = importar_pesos_bias()\n",
    "\n",
    "input_size = 16#X_train.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 7\n",
    "\n",
    "perceptron = PerceptronMulticapa(input_size, hidden_size, output_size,weights_input_hidden,weights_hidden_output,bias_hidden,bias_output)\n",
    "# esta inicializacion de abajo fue usada previamente. Aca, no se pasan por parametros los pesos y bias. Por lo tanto, se inicializan aleatoriamente\n",
    "# perceptron = PerceptronMulticapa(input_size, hidden_size, output_size)\n",
    "\n",
    "perceptron.entrenar(X_train, y_train, epocas=1000, ratio_aprendizaje=0.01,debug=True)\n",
    "\n",
    "predicciones = perceptron.predecir(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0fce3-4770-4b1d-b465-76bac9adcefc",
   "metadata": {},
   "source": [
    "## 6. Analisis del accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec26e7d7-cd12-47bd-b7c1-02149fcf2b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valor predecido:  1  - valor real:  [5.]\n",
      "valor predecido:  1  - valor real:  [2.]\n",
      "valor predecido:  1  - valor real:  [4.]\n",
      "valor predecido:  1  - valor real:  [2.]\n",
      "valor predecido:  1  - valor real:  [1.]\n",
      "valor predecido:  1  - valor real:  [1.]\n",
      "valor predecido:  1  - valor real:  [4.]\n",
      "valor predecido:  6  - valor real:  [6.]\n",
      "valor predecido:  1  - valor real:  [1.]\n",
      "valor predecido:  1  - valor real:  [4.]\n",
      "valor predecido:  3  - valor real:  [7.]\n",
      "valor predecido:  1  - valor real:  [1.]\n",
      "valor predecido:  1  - valor real:  [1.]\n",
      "valor predecido:  6  - valor real:  [7.]\n",
      "valor predecido:  1  - valor real:  [1.]\n",
      "valor predecido:  1  - valor real:  [2.]\n",
      "valor predecido:  1  - valor real:  [3.]\n",
      "valor predecido:  6  - valor real:  [6.]\n",
      "valor predecido:  1  - valor real:  [1.]\n",
      "valor predecido:  1  - valor real:  [1.]\n",
      "valor predecido:  1  - valor real:  [2.]\n",
      "accuracy:  0.47619047619047616\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for itr,pred in enumerate(predicciones):\n",
    "    if(np.argmax(pred)+1 == y_test[itr]*7):\n",
    "        total += 1\n",
    "    print('valor predecido: ',np.argmax(pred)+1,' - valor real: ',y_test[itr]*7)\n",
    "print('accuracy: ',total/len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
